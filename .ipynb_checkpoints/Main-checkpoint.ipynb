{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c9ebeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/site-packages (9.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db3b6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a78ae",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dbc2c37",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '319_4.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m dataPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculating mean and std for normalization\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m rgb_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate( [Image\u001b[38;5;241m.\u001b[39mopen(img)\u001b[38;5;241m.\u001b[39mgetdata() \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dataPath)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# rgb_values.shape == (n, 3), \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# where n is the total number of pixels in all images, \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# and 3 are the 3 channels: R, G, B.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Each value is in the interval [0; 1]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m mu_rgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(rgb_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# mu_rgb.shape == (3,)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m dataPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculating mean and std for normalization\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m rgb_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate( [\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgetdata() \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dataPath)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# rgb_values.shape == (n, 3), \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# where n is the total number of pixels in all images, \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# and 3 are the 3 channels: R, G, B.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Each value is in the interval [0; 1]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m mu_rgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(rgb_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# mu_rgb.shape == (3,)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/PIL/Image.py:3068\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3065\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3068\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3069\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '319_4.jpg'"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "dataPath = \"Data/\"\n",
    "\n",
    "# Calculating mean and std for normalization\n",
    "rgb_values = np.concatenate( [Image.open(img).getdata() for img in os.listdir(dataPath)], axis=0) / 255.\n",
    "\n",
    "# rgb_values.shape == (n, 3), \n",
    "# where n is the total number of pixels in all images, \n",
    "# and 3 are the 3 channels: R, G, B.\n",
    "\n",
    "# Each value is in the interval [0; 1]\n",
    "\n",
    "mu_rgb = np.mean(rgb_values, axis=0)  # mu_rgb.shape == (3,)\n",
    "std_rgb = np.std(rgb_values, axis=0)  # std_rgb.shape == (3,)\n",
    "\n",
    "print(mu_rgb, std_rgb)\n",
    "class SneakersDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_csv = pd.read_csv(csv_file)\n",
    "        self.img_dir = root_dir\n",
    "        #self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_csv)-1\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        #all normalization and data augmentation steps should be applied to the image before this method returns it.\n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        '''\n",
    "        \n",
    "        file = os.path.join(self.img_dir, self.data_csv.iloc[idx, 0])\n",
    "        imagepil = PIL.Image.open(os.path.join(self.img_dir, file)).convert('RGB')\n",
    "        \n",
    "        #convert image to Tensor and normalize\n",
    "        image = utils.to_tensor_and_normalize(imagepil)\n",
    "        \n",
    "\n",
    "        label = torch.Tensor(self.all_labels.loc[selected_filename,:].values)\n",
    "        sample = {'data':image, #preprocessed image, for input into NN\n",
    "                  'label':label,\n",
    "                  'img_idx':idx}\n",
    "\n",
    "        return sample\n",
    "\n",
    "dataset = SneakersDataset(csv_file='dataInfo.csv', root_dir='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641a1e9",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943bfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4a7cdff",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c05a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b7b1f7c",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1892119e",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e2892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e305bdc",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d556c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
